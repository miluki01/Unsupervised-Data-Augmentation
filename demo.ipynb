{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import copy\n",
    "import json\n",
    "import torch\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from augmentation import TfIdfAugmentation\n",
    "from tools.utils import ExternalPreprocessor\n",
    "\n",
    "from modelling.models import DAN, Embedding\n",
    "from modelling.templates import SequenceTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_PATH = '../routing/data/sberbank_embeddings/w2v_m5_w3_v300_norm_v48_vocab.txt'\n",
    "W2V_MATRIX_PATH = '../routing/data/sberbank_embeddings/w2v_m5_w3_v300_norm_v48_vectors.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('token2prob.json') as f:\n",
    "    token2prob = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VOCAB_PATH) as f:\n",
    "    vocab = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {key: value for value, key in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2prob = {vocab[token]: token2prob[token] for token in token2prob if token in vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = TfIdfAugmentation(indexes_matrix=np.load('nearest_matrix.npy'), index2prob=index2prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.jsonl') as f:\n",
    "    data = [json.loads(sample) for sample in f.read().split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data[:-len(data) // 10], data[-len(data) // 10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11623, 1292, True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test), len(train) + len(test) == len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET2INDEX = {\n",
    "    'ANNA.1.sales': 0,\n",
    "    'ANNA.1.sbbol': 1,\n",
    "    'ANNA.1.oper_support': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_padding(sequence, max_sequence_length, value) -> np.ndarray:\n",
    "\n",
    "    sequence = sequence[:max_sequence_length]\n",
    "\n",
    "    if len(sequence) < max_sequence_length:\n",
    "        for _ in range((max_sequence_length - len(sequence))):\n",
    "            sequence.append(value)\n",
    "\n",
    "    sequence = np.array(sequence)\n",
    "\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexing_batch(x, vocab, max_sequence_length):\n",
    "    \n",
    "    x = [[vocab[tok] for tok in sample if tok in vocab] for sample in x]\n",
    "    \n",
    "    x = np.array([sequence_padding(sample, max_sequence_length=max_sequence_length, value=0) for sample in x])\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_processing(batch):\n",
    "    \n",
    "    x = [sample['tokens'] for sample in batch]\n",
    "    \n",
    "    x = indexing_batch(x, vocab, MAX_LEN)\n",
    "    x_aug = aug.replace_batch(copy.deepcopy(x))\n",
    "    \n",
    "    y = np.array([TARGET2INDEX[sample['target']] for sample in batch])\n",
    "    \n",
    "    x = torch.LongTensor(x)\n",
    "    x_aug = torch.LongTensor(x_aug)\n",
    "    y = torch.LongTensor(y)\n",
    "    \n",
    "    return [x, x_aug, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(data, batch_size=32):\n",
    "\n",
    "    for n_batch in range(math.ceil(len(data) / batch_size)):\n",
    "\n",
    "        batch = data[n_batch * batch_size:(n_batch + 1) * batch_size]\n",
    "\n",
    "        batch = batch_processing(batch)\n",
    "\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelling.layers import BaseModule\n",
    "from modelling.templates import SequenceTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_matrix = np.load(W2V_MATRIX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(BaseModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = Embedding(vocab_size=word_matrix.shape[0],\n",
    "                                   embedding_matrix=word_matrix)\n",
    "        \n",
    "        self.dan = DAN((300, 256), activation_function_output=torch.nn.ReLU())\n",
    "        \n",
    "        self.linear = torch.nn.Linear(256, 256)\n",
    "        \n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "        self.classifier = torch.nn.Linear(256, 3)\n",
    "        \n",
    "    def forward(self, x, x_aug):\n",
    "        \n",
    "        x_rep = self.embedding(x)\n",
    "        x_rep = self.dan(x_rep)\n",
    "        x_rep = self.linear(x_rep)\n",
    "        x_rep = torch.nn.functional.log_softmax(x_rep, dim=1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            x_aug_rep = self.embedding(x_aug)\n",
    "            x_aug_rep = self.dan(x_aug_rep)\n",
    "            x_aug_rep = self.linear(x_aug_rep)\n",
    "            x_aug_rep = torch.nn.functional.softmax(x_aug_rep, dim=1)\n",
    "    \n",
    "        y_pred = self.classifier(x_rep)\n",
    "        \n",
    "        return x_rep, x_aug_rep, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_div = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "cross_entropy = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0: 100%|██████████| 11623/11623 [00:02<00:00, 4602.69it/s, ce_loss=1.26, kl_loss=0.000572, loss=1.26]  \n",
      "Epoch: 1:   8%|▊         | 896/11623 [00:00<00:02, 4628.95it/s, ce_loss=0.757, kl_loss=0.0016, loss=0.773]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Train - 0.001 | Test - 0.001\n",
      "CE Train - 1.521 | Test - 0.683\n",
      "Aggregated Train - 1.527 | Test - 0.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 11623/11623 [00:02<00:00, 4566.01it/s, ce_loss=1.34, kl_loss=0.00148, loss=1.36]  \n",
      "Epoch: 2:   8%|▊         | 896/11623 [00:00<00:02, 4673.08it/s, ce_loss=0.715, kl_loss=0.00213, loss=0.736]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Train - 0.002 | Test - 0.002\n",
      "CE Train - 0.642 | Test - 0.581\n",
      "Aggregated Train - 0.660 | Test - 0.583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 11623/11623 [00:02<00:00, 4598.71it/s, ce_loss=1.34, kl_loss=0.00187, loss=1.36]  \n",
      "Epoch: 3:   8%|▊         | 896/11623 [00:00<00:02, 4620.97it/s, ce_loss=0.699, kl_loss=0.00191, loss=0.718]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Train - 0.002 | Test - 0.002\n",
      "CE Train - 0.597 | Test - 0.541\n",
      "Aggregated Train - 0.621 | Test - 0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 11623/11623 [00:02<00:00, 4590.44it/s, ce_loss=1.31, kl_loss=0.00212, loss=1.33]  \n",
      "Epoch: 4:   8%|▊         | 896/11623 [00:00<00:02, 4671.20it/s, ce_loss=0.684, kl_loss=0.00195, loss=0.703]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Train - 0.003 | Test - 0.002\n",
      "CE Train - 0.577 | Test - 0.518\n",
      "Aggregated Train - 0.602 | Test - 0.520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 11623/11623 [00:02<00:00, 4589.88it/s, ce_loss=1.27, kl_loss=0.00184, loss=1.29]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Train - 0.003 | Test - 0.003\n",
      "CE Train - 0.561 | Test - 0.499\n",
      "Aggregated Train - 0.587 | Test - 0.503\n"
     ]
    }
   ],
   "source": [
    "kl_losses = []\n",
    "ce_losses = []\n",
    "losses = []\n",
    "\n",
    "l = 0.5\n",
    "\n",
    "for n in range(5):\n",
    "    \n",
    "    epoch_kl_losses = []\n",
    "    epoch_ce_losses = []\n",
    "    epoch_losses = []\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    pg = tqdm(total=len(train), desc=f'Epoch: {n}')\n",
    "\n",
    "    for x, x_aug, y in loader(train):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_rep, x_aug_rep, y_pred = model(x, x_aug)\n",
    "\n",
    "        kl_loss = kl_div(x_rep, x_aug_rep)\n",
    "        ce_loss = cross_entropy(y_pred, y)\n",
    "\n",
    "        loss = ce_loss + 10 * kl_loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_kl_losses.append(kl_loss.item())\n",
    "        epoch_ce_losses.append(ce_loss.item())\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "        pg.update(x.shape[0])\n",
    "        pg.set_postfix(kl_loss=epoch_kl_losses[-1], ce_loss=epoch_ce_losses[-1], loss=epoch_losses[-1])\n",
    "\n",
    "    pg.close()\n",
    "    \n",
    "    test_epoch_kl_losses = []\n",
    "    test_epoch_ce_losses = []\n",
    "    test_epoch_losses = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for x, x_aug, y in loader(test):\n",
    "\n",
    "        x_rep, x_aug_rep, y_pred = model(x, x_aug)\n",
    "\n",
    "        kl_loss = kl_div(x_rep, x_aug_rep)\n",
    "        ce_loss = cross_entropy(y_pred, y)\n",
    "\n",
    "        loss = ce_loss + kl_loss\n",
    "\n",
    "    test_epoch_kl_losses.append(kl_loss.item())\n",
    "    test_epoch_ce_losses.append(ce_loss.item())\n",
    "    test_epoch_losses.append(loss.item())\n",
    "    \n",
    "    print('KL Train - {:.3f} | Test - {:.3f}'.format(np.mean(epoch_kl_losses), np.mean(test_epoch_kl_losses)))\n",
    "    print('CE Train - {:.3f} | Test - {:.3f}'.format(np.mean(epoch_ce_losses), np.mean(test_epoch_ce_losses)))\n",
    "    print('Aggregated Train - {:.3f} | Test - {:.3f}'.format(np.mean(epoch_losses), np.mean(test_epoch_losses)))\n",
    "    \n",
    "    kl_losses.extend(copy.deepcopy(epoch_kl_losses))\n",
    "    ce_losses.extend(copy.deepcopy(epoch_ce_losses))\n",
    "    losses.extend(copy.deepcopy(epoch_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(BaseModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = Embedding(vocab_size=word_matrix.shape[0],\n",
    "                                   embedding_matrix=word_matrix)\n",
    "        \n",
    "        self.dan = DAN((300, 256), activation_function_output=torch.nn.ReLU())\n",
    "        \n",
    "        self.linear = torch.nn.Linear(256, 256)\n",
    "        \n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "        self.classifier = torch.nn.Linear(256, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_rep = self.embedding(x)\n",
    "        x_rep = self.dan(x_rep)\n",
    "        x_rep = self.linear(x_rep)\n",
    "        x_rep = torch.nn.functional.log_softmax(x_rep, dim=1)\n",
    "    \n",
    "        y_pred = self.classifier(x_rep)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0: 100%|██████████| 11623/11623 [00:02<00:00, 5136.96it/s, loss=1.48] \n",
      "Epoch: 1:   9%|▉         | 1024/11623 [00:00<00:02, 5239.85it/s, loss=0.502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Train - 0.805 | Test - 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 11623/11623 [00:02<00:00, 5149.50it/s, loss=1.58] \n",
      "Epoch: 2:   9%|▉         | 1024/11623 [00:00<00:02, 5291.72it/s, loss=0.471]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Train - 0.610 | Test - 0.536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 11623/11623 [00:02<00:00, 5137.20it/s, loss=1.53] \n",
      "Epoch: 3:   9%|▉         | 1024/11623 [00:00<00:02, 5242.93it/s, loss=0.462]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Train - 0.578 | Test - 0.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 11623/11623 [00:02<00:00, 5144.45it/s, loss=1.46] \n",
      "Epoch: 4:   9%|▊         | 992/11623 [00:00<00:02, 5108.51it/s, loss=0.474]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Train - 0.559 | Test - 0.468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 11623/11623 [00:02<00:00, 5114.81it/s, loss=1.39] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Train - 0.544 | Test - 0.451\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "l = 0.5\n",
    "\n",
    "for n in range(5):\n",
    "    \n",
    "    epoch_losses = []\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    pg = tqdm(total=len(train), desc=f'Epoch: {n}')\n",
    "\n",
    "    for x, _, y in loader(train):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = cross_entropy(y_pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "        pg.update(x.shape[0])\n",
    "        pg.set_postfix(loss=epoch_losses[-1])\n",
    "\n",
    "    pg.close()\n",
    "\n",
    "    test_epoch_losses = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for x, x_aug, y in loader(test):\n",
    "\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = cross_entropy(y_pred, y)\n",
    "\n",
    "    test_epoch_losses.append(loss.item())\n",
    "    \n",
    "    print('Loss Train - {:.3f} | Test - {:.3f}'.format(np.mean(epoch_losses), np.mean(test_epoch_losses)))\n",
    "\n",
    "    losses.extend(copy.deepcopy(epoch_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
